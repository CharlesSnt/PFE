import numpy as np
import scipy.io
import torch
import torch.nn as nn
import torch.optim as optim
import torch.autograd as autograd
from torch.nn.utils import parameters_to_vector

# -----------------------------
# Device
# -----------------------------
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

np.random.seed(1234)
torch.manual_seed(1234)


# -----------------------------
# Initialization
# -----------------------------
def init_weights(m):
    if isinstance(m, (nn.Conv2d, nn.Linear)):
        nn.init.xavier_normal_(m.weight)
        if m.bias is not None:
            nn.init.zeros_(m.bias)


# -----------------------------
# Model Class
# -----------------------------
class PhysicsInformedCNN(nn.Module):
    def __init__(self, in_channels=3, hidden_channels=32, out_channels=3):
        super().__init__()
        self.conv1 = nn.Conv2d(in_channels, hidden_channels, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(
            hidden_channels, hidden_channels, kernel_size=3, padding=1
        )
        self.conv3 = nn.Conv2d(
            hidden_channels, hidden_channels, kernel_size=3, padding=1
        )
        self.out_conv = nn.Conv2d(hidden_channels, out_channels, kernel_size=1)
        self.apply(init_weights)

    def forward(self, xyt):
        h = torch.tanh(self.conv1(xyt))
        h = torch.tanh(self.conv2(h))
        h = torch.tanh(self.conv3(h))
        out = self.out_conv(h)
        return out[:, 0:1, :, :], out[:, 1:2, :, :], out[:, 2:3, :, :]


# -----------------------------
# Physics (Navier-Stokes)
# -----------------------------
def navier_stokes_residual(model, xyt):
    lambda_1 = torch.tensor([0.0], dtype=torch.float32, requires_grad=True)
    lambda_2 = torch.tensor([0.0], dtype=torch.float32, requires_grad=True)
    xyt.requires_grad_(True)
    u, v, p = model(xyt)

    u_g = autograd.grad(u, xyt, grad_outputs=torch.ones_like(u), create_graph=True)[0]
    u_x, u_y, u_t = u_g[:, 0:1], u_g[:, 1:2], u_g[:, 2:3]

    v_g = autograd.grad(v, xyt, grad_outputs=torch.ones_like(v), create_graph=True)[0]
    v_x, v_y, v_t = v_g[:, 0:1], v_g[:, 1:2], v_g[:, 2:3]

    p_g = autograd.grad(p, xyt, grad_outputs=torch.ones_like(p), create_graph=True)[0]
    p_x, p_y = p_g[:, 0:1], p_g[:, 1:2]

    u_xx = autograd.grad(
        u_x, xyt, grad_outputs=torch.ones_like(u_x), create_graph=True
    )[0][:, 0:1]
    u_yy = autograd.grad(
        u_y, xyt, grad_outputs=torch.ones_like(u_y), create_graph=True
    )[0][:, 1:2]

    v_xx = autograd.grad(
        v_x, xyt, grad_outputs=torch.ones_like(v_x), create_graph=True
    )[0][:, 0:1]
    v_yy = autograd.grad(
        v_y, xyt, grad_outputs=torch.ones_like(v_y), create_graph=True
    )[0][:, 1:2]

    f_u = u_t + lambda_1*(u * u_x + v * u_y) + p_x - lambda_2 * (u_xx + u_yy)
    f_v = v_t + lambda_1*(u * v_x + v * v_y) + p_y - lambda_2 * (v_xx + v_yy)

    return f_u, f_v, u, v, p, lambda_1, lambda_2


def pinn_loss(model, xyt, u_true, v_true):
    f_u, f_v, u_pred, v_pred, _, _, _ = navier_stokes_residual(model, xyt)
    mse_data = ((u_pred - u_true) ** 2).mean() + ((v_pred - v_true) ** 2).mean()
    mse_pde = (f_u**2).mean() + (f_v**2).mean()
    return mse_data + mse_pde


# -----------------------------
# Generic Training Function
# -----------------------------
def run_training_session(mode_name, xyt, u_true, v_true, steps=2000, use_lbfgs=False):
    """
    Lance un entraînement complet selon le mode spécifié.
    Modes: 'adam', 'sgd'
    """
    print(f"\n=== Start to train : {mode_name} (LBFGS={use_lbfgs}) ===")

    model = PhysicsInformedCNN(in_channels=3, hidden_channels=32, out_channels=3).to(
        device
    )
    model.apply(init_weights)

    if "sgd" in mode_name.lower():
        # SGD might need a higher learning rate
        optimizer = optim.SGD(model.parameters(), lr=1e-2)
    else:
        # default Adam
        optimizer = optim.Adam(model.parameters(), lr=1e-3)

    weight_history = []

    # --- Principal loop (Adam / SGD) ---
    for it in range(steps):
        optimizer.zero_grad()
        loss = pinn_loss(model, xyt, u_true, v_true)
        loss.backward()
        optimizer.step()

        if it % 50 == 0:
            current_w = parameters_to_vector(model.parameters()).detach().cpu()#si on enlève cpu alors se sera sur gpu
            weight_history.append(current_w)
            if it % 500 == 0:
                print(f"Iter {it}, Loss: {loss.item():.3e}")

    weight_history.append(parameters_to_vector(model.parameters()).detach().cpu())

    # --- Phase LBFGS (Optionnal) ---
    if use_lbfgs:
        print("Début LBFGS...")
        optimizer_lbfgs = optim.LBFGS(
            model.parameters(),
            max_iter=500,
            tolerance_grad=1e-9,
            tolerance_change=1e-9,
            history_size=50,
            line_search_fn="strong_wolfe",
        )

        def closure():
            optimizer_lbfgs.zero_grad()
            loss = pinn_loss(model, xyt, u_true, v_true)
            loss.backward()
            return loss

        optimizer_lbfgs.step(closure)

        weight_history.append(parameters_to_vector(model.parameters()).detach().cpu())

    # Save the loss functions
    suffix = "_lbfgs" if use_lbfgs else ""
    filename_base = f"navier_{mode_name}{suffix}"

    torch.save(model.state_dict(), f"{filename_base}_model.pth")
    torch.save(torch.stack(weight_history), f"{filename_base}_traj.pt")
    print(f"Sauvegardé : {filename_base}_model.pth et .pt")


# -----------------------------
# MAIN
# -----------------------------
if __name__ == "__main__":
    # Load Data
    data = scipy.io.loadmat("PFE/Data/cylinder_nektar_wake.mat")
    U_star = data["U_star"]
    t_star = data["t"]
    X_star = data["X_star"]

    N = X_star.shape[0]
    T = t_star.shape[0]

    XX = np.tile(X_star[:, 0:1], (1, T))
    YY = np.tile(X_star[:, 1:2], (1, T))
    TT = np.tile(t_star, (1, N)).T
    UU = U_star[:, 0, :]
    VV = U_star[:, 1, :]

    x = XX.flatten()[:, None]
    y = YY.flatten()[:, None]
    t = TT.flatten()[:, None]
    u = UU.flatten()[:, None]
    v = VV.flatten()[:, None]

    # Training Data Selection
    N_train = 5000
    idx = np.random.choice(N * T, N_train, replace=False)

    x_train = torch.tensor(x[idx, :], dtype=torch.float32)
    y_train = torch.tensor(y[idx, :], dtype=torch.float32)
    t_train = torch.tensor(t[idx, :], dtype=torch.float32)
    u_train = torch.tensor(u[idx, :], dtype=torch.float32)
    v_train = torch.tensor(v[idx, :], dtype=torch.float32)

    xyt_train = (
        torch.cat([x_train, y_train, t_train], dim=1).unsqueeze(-1).unsqueeze(-1)
    )
    xyt_train.requires_grad_(True)
    u_train = u_train.unsqueeze(-1).unsqueeze(-1)
    v_train = v_train.unsqueeze(-1).unsqueeze(-1)

    xyt_train = xyt_train.to(device)
    u_train = u_train.to(device)
    v_train = v_train.to(device)

    # --- Different trainings ---

    # 1. Adam + LBFGS (Original one)
    run_training_session(
        "adam", xyt_train, u_train, v_train, steps=1000, use_lbfgs=True
    )

    # 2. Adam seul (Without LBFGS)
    run_training_session(
        "adam_only", xyt_train, u_train, v_train, steps=2000, use_lbfgs=False
    )

    # 3. Gradient Descent (SGD)
    run_training_session(
        "sgd", xyt_train, u_train, v_train, steps=2000, use_lbfgs=False
    )

    print("\nTraining finished !")
